{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSqYnQ0tD2Kn",
        "outputId": "4e28c7cc-3475-4b66-9aa3-a70d3de65377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr  1 08:40:14 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from matplotlib import pyplot as plt\n",
        "from IPython.display import display, Image\n"
      ],
      "metadata": {
        "id": "YIaM_1GwNN9A"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKF9m9UwyanB",
        "outputId": "3cbb7004-aec6-4a12-e432-5f8cce555712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patool --quiet\n",
        "import patoolib\n",
        "patoolib.extract_archive(\"/content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar\", outdir=\"/content/dataset\")"
      ],
      "metadata": {
        "id": "HHQZTvPeyrOW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "outputId": "765932bf-dffe-4f10-8b35-1b0ee4d1961e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m71.7/96.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.0/96.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar ...\n",
            "INFO:patool:Extracting /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar ...\n",
            "INFO patool: ... creating output directory `/content/dataset'.\n",
            "INFO:patool:... creating output directory `/content/dataset'.\n",
            "INFO patool: running /usr/bin/unrar x -- /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar\n",
            "INFO:patool:running /usr/bin/unrar x -- /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar\n",
            "INFO patool:     with cwd='/content/dataset', input=''\n",
            "INFO:patool:    with cwd='/content/dataset', input=''\n",
            "INFO patool: ... /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar extracted to `/content/dataset'.\n",
            "INFO:patool:... /content/drive/MyDrive/AI/SK_dataset/melanoma_cancer_dataset.rar extracted to `/content/dataset'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/dataset'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prevent exponentially memory growth\n",
        "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
        "try:\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "  pass"
      ],
      "metadata": {
        "id": "Se-Rv8MMNafd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgVQTUk8EGcQ",
        "outputId": "01c5aa2a-f1bf-4091-c00e-8033eaa315fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install roboflow --quiet\n",
        "\n",
        "#from roboflow import Roboflow\n",
        "#rf = Roboflow(api_key=\"EX21p6P3sYw3SwyBkyQQ\")\n",
        "#project = rf.workspace(\"martin-rpfil\").project(\"skin-cancer-ihubf\")\n",
        "#version = project.version(1)\n",
        "#dataset = version.download(\"folder\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CmiSiXWEbiv",
        "outputId": "e7498f82-9015-4854-9834-b33156698f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Paths configuration\n",
        "base_dir = '/content/dataset/melanoma_cancer_dataset'  # Update this to your dataset's path\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')  # Assuming you already have this\n",
        "validation_dir = os.path.join(base_dir, 'valid')  # Validation directory to create\n",
        "\n",
        "classes = ['benign', 'malignant']  # Class names\n",
        "\n",
        "# Create validation directory and class subdirectories\n",
        "if not os.path.exists(validation_dir):\n",
        "    os.mkdir(validation_dir)\n",
        "    for cls in classes:\n",
        "        os.mkdir(os.path.join(validation_dir, cls))\n",
        "\n",
        "# Split ratio configuration\n",
        "split_ratio = 0.2  # 20% for validation\n",
        "\n",
        "# Move a subset of images for each class from training to validation\n",
        "for cls in classes:\n",
        "    class_dir = os.path.join(train_dir, cls)\n",
        "    images = os.listdir(class_dir)\n",
        "\n",
        "    # Split images into training and validation\n",
        "    train_imgs, valid_imgs = train_test_split(images, test_size=split_ratio, random_state=42)\n",
        "\n",
        "    # Move validation images to validation directory\n",
        "    for img in valid_imgs:\n",
        "        src_path = os.path.join(class_dir, img)\n",
        "        dest_path = os.path.join(validation_dir, cls, img)\n",
        "        shutil.move(src_path, dest_path)\n",
        "\n",
        "print(\"Data split into training and validation sets.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH-ZhkaL0RQe",
        "outputId": "4c3daf6e-a782-45e4-b8a6-192c4a31cbd9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into training and validation sets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define directories for your dataset\n",
        "base_dir = '/content/dataset/melanoma_cancer_dataset/'\n",
        "train_dir = f'{base_dir}/train'\n",
        "validation_dir = f'{base_dir}/valid'\n",
        "test_dir = f'{base_dir}/test'\n",
        "\n",
        "# Initialize the data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        "    )\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)  # No augmentation for validation/test data\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(224, 224),  # Resize images to fit MobileNetV2 input requirements\n",
        "    batch_size=64,\n",
        "    class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='binary')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=64,\n",
        "    class_mode='binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h7Zd9DZFPDF",
        "outputId": "9b7d1c0b-f7ff-4cf5-f543-72d67c72b249"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7684 images belonging to 2 classes.\n",
            "Found 1921 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The ideal Learning rate, optimizer and layers to unfreeze for fine-tuning"
      ],
      "metadata": {
        "id": "RcdJdHimWxu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self, min_lr=1e-7, max_lr=1, steps_per_epoch=None, epochs=None):\n",
        "        super().__init__()\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.total_iterations = steps_per_epoch * epochs\n",
        "        self.iteration = 0\n",
        "        self.history = {}\n",
        "\n",
        "    def clr(self):\n",
        "        \"\"\" Calculate the learning rate. \"\"\"\n",
        "        x = self.iteration / self.total_iterations\n",
        "        return self.min_lr + (self.max_lr-self.min_lr) * x\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        \"\"\" Initialize the learning rate to the minimum value at the start of training. \"\"\"\n",
        "        logs = logs or {}\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, self.min_lr)\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \"\"\" Update the learning rate and record the current learning rate and loss. \"\"\"\n",
        "        logs = logs or {}\n",
        "        self.iteration += 1\n",
        "\n",
        "        lr = self.clr()\n",
        "        self.history.setdefault('lr', []).append(lr)\n",
        "        self.history.setdefault('loss', []).append(logs.get('loss'))\n",
        "\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "# Example usage\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "\n",
        "lr_finder = LRFinder(min_lr=1e-7, max_lr=1, steps_per_epoch=np.ceil(train_generator.samples/train_generator.batch_size), epochs=3)\n",
        "\n",
        "model.fit(train_generator, steps_per_epoch=np.ceil(train_generator.samples/train_generator.batch_size), epochs=3, callbacks=[lr_finder])\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(lr_finder.history['lr'], lr_finder.history['loss'])\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning rate')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning rate vs. Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "Q7P8eGoeW_F6",
        "outputId": "d9992f28-658f-4412-e89c-054faa554bbc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Epoch 1/3\n",
            "61/61 [==============================] - 146s 2s/step - loss: 1.2867\n",
            "Epoch 2/3\n",
            "61/61 [==============================] - 106s 2s/step - loss: 0.9529\n",
            "Epoch 3/3\n",
            "61/61 [==============================] - 106s 2s/step - loss: 0.4858\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "import math\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# Define your parameter grid\n",
        "learning_rates = [1e-2, 5e-2, 1e-1]\n",
        "optimizers = [Adam, SGD, RMSprop]\n",
        "fine_tune_layers = [50, 80, 110, 153]  # Starting points for fine-tuning\n",
        "\n",
        "# Placeholder for recording the results\n",
        "results = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for opt_class in optimizers:\n",
        "        for fine_tune_at in fine_tune_layers:\n",
        "            print(f\"Training with LR: {lr}, Optimizer: {opt_class.__name__}, Fine-tuning at layer: {fine_tune_at}\")\n",
        "\n",
        "            # Reinitialize the model with MobileNetV2 as the base\n",
        "            base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "            base_model.trainable = False\n",
        "\n",
        "            model = Sequential([\n",
        "                base_model,\n",
        "                GlobalAveragePooling2D(),\n",
        "                Dropout(0.2),\n",
        "                Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "\n",
        "            optimizer = opt_class(learning_rate=lr)\n",
        "            model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "                        # Function for step decay schedule\n",
        "            def step_decay(epoch):\n",
        "                initial_lr = 0.001  # Starting learning rate\n",
        "                drop = 0.5  # Reduce learning rate by half\n",
        "                epochs_drop = 10.0  # Reduce every 10 epochs\n",
        "                lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "                return lr\n",
        "\n",
        "            # Define the Early Stopping callback\n",
        "            early_stopping = EarlyStopping(\n",
        "                monitor='val_loss',  # Monitor validation loss\n",
        "                patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "                restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored metric\n",
        "\n",
        "            # Set up callbacks\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
        "            model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "            lr_scheduler = LearningRateScheduler(step_decay)\n",
        "\n",
        "            # Fit the model\n",
        "            history = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "                epochs=30,  # Adjust epochs based on your needs\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "                callbacks=[lr_scheduler, early_stopping, model_checkpoint]\n",
        "            )\n",
        "\n",
        "            # Unfreeze the top layers for fine-tuning\n",
        "            base_model.trainable = True\n",
        "            for layer in base_model.layers[:fine_tune_at]:\n",
        "                layer.trainable = False\n",
        "\n",
        "            # Compile the model for fine-tuning with a lower learning rate\n",
        "            model.compile(optimizer=opt_class(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            # Fit the model again (fine-tuning)\n",
        "            history_fine = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "                epochs=20,  # Adjust based on your needs\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "                callbacks=[early_stopping, lr_scheduler, model_checkpoint]\n",
        "            )\n",
        "\n",
        "            # Get the best validation accuracy from the fine-tuning phase\n",
        "            best_val_accuracy = max(history_fine.history['val_accuracy'])\n",
        "            print(f\"Best validation accuracy: {best_val_accuracy}\")\n",
        "\n",
        "            # Record the results\n",
        "            results.append({\n",
        "                'lr': lr,\n",
        "                'optimizer': opt_class.__name__,\n",
        "                'fine_tune_at': fine_tune_at,\n",
        "                'val_accuracy': best_val_accuracy\n",
        "            })\n",
        "\n",
        "# Write results to a file or print them out\n",
        "with open('model_tuning_results.txt', 'w') as f:\n",
        "    for result in results:\n",
        "        f.write(f\"LR: {result['lr']}, Optimizer: {result['optimizer']}, Fine-tune Start Layer: {result['fine_tune_at']}, \"\n",
        "                f\"Best Val Accuracy: {result['val_accuracy']:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MLGyvZMWAAc",
        "outputId": "d8d313d5-373e-44d4-ee60-31cda50d8c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR: 0.01, Optimizer: Adam, Fine-tuning at layer: 50\n",
            "Epoch 1/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4430 - accuracy: 0.7914\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84167, saving model to best_model.h5\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60/60 [==============================] - 105s 2s/step - loss: 0.4430 - accuracy: 0.7914 - val_loss: 0.3513 - val_accuracy: 0.8417 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3449 - accuracy: 0.8526\n",
            "Epoch 2: val_accuracy improved from 0.84167 to 0.85521, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3449 - accuracy: 0.8526 - val_loss: 0.3279 - val_accuracy: 0.8552 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8665\n",
            "Epoch 3: val_accuracy improved from 0.85521 to 0.86667, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3149 - accuracy: 0.8665 - val_loss: 0.3115 - val_accuracy: 0.8667 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3140 - accuracy: 0.8673\n",
            "Epoch 4: val_accuracy improved from 0.86667 to 0.87031, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3140 - accuracy: 0.8673 - val_loss: 0.3061 - val_accuracy: 0.8703 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2986 - accuracy: 0.8723\n",
            "Epoch 5: val_accuracy improved from 0.87031 to 0.87187, saving model to best_model.h5\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.2986 - accuracy: 0.8723 - val_loss: 0.2992 - val_accuracy: 0.8719 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2913 - accuracy: 0.8719\n",
            "Epoch 6: val_accuracy improved from 0.87187 to 0.87396, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2913 - accuracy: 0.8719 - val_loss: 0.2967 - val_accuracy: 0.8740 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2896 - accuracy: 0.8768\n",
            "Epoch 7: val_accuracy improved from 0.87396 to 0.88177, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2896 - accuracy: 0.8768 - val_loss: 0.2935 - val_accuracy: 0.8818 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2831 - accuracy: 0.8818\n",
            "Epoch 8: val_accuracy improved from 0.88177 to 0.88229, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2831 - accuracy: 0.8818 - val_loss: 0.2908 - val_accuracy: 0.8823 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2822 - accuracy: 0.8806\n",
            "Epoch 9: val_accuracy did not improve from 0.88229\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.2822 - accuracy: 0.8806 - val_loss: 0.2907 - val_accuracy: 0.8818 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2780 - accuracy: 0.8899\n",
            "Epoch 10: val_accuracy improved from 0.88229 to 0.88490, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2780 - accuracy: 0.8899 - val_loss: 0.2891 - val_accuracy: 0.8849 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2807 - accuracy: 0.8849\n",
            "Epoch 11: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2807 - accuracy: 0.8849 - val_loss: 0.2868 - val_accuracy: 0.8849 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.8853\n",
            "Epoch 12: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2779 - accuracy: 0.8853 - val_loss: 0.2872 - val_accuracy: 0.8828 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2792 - accuracy: 0.8842\n",
            "Epoch 13: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.2792 - accuracy: 0.8842 - val_loss: 0.2854 - val_accuracy: 0.8786 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2757 - accuracy: 0.8847\n",
            "Epoch 14: val_accuracy improved from 0.88490 to 0.88594, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2757 - accuracy: 0.8847 - val_loss: 0.2845 - val_accuracy: 0.8859 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.8818\n",
            "Epoch 15: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2754 - accuracy: 0.8818 - val_loss: 0.2821 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2762 - accuracy: 0.8859\n",
            "Epoch 16: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2762 - accuracy: 0.8859 - val_loss: 0.2858 - val_accuracy: 0.8859 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8850\n",
            "Epoch 17: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2770 - accuracy: 0.8850 - val_loss: 0.2825 - val_accuracy: 0.8859 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8867\n",
            "Epoch 18: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2737 - accuracy: 0.8867 - val_loss: 0.2831 - val_accuracy: 0.8823 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2711 - accuracy: 0.8838\n",
            "Epoch 19: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2711 - accuracy: 0.8838 - val_loss: 0.2873 - val_accuracy: 0.8844 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2745 - accuracy: 0.8833\n",
            "Epoch 20: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2745 - accuracy: 0.8833 - val_loss: 0.2808 - val_accuracy: 0.8849 - lr: 2.5000e-04\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2700 - accuracy: 0.8872\n",
            "Epoch 21: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2700 - accuracy: 0.8872 - val_loss: 0.2794 - val_accuracy: 0.8854 - lr: 2.5000e-04\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.8880\n",
            "Epoch 22: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2690 - accuracy: 0.8880 - val_loss: 0.2798 - val_accuracy: 0.8854 - lr: 2.5000e-04\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2654 - accuracy: 0.8920\n",
            "Epoch 23: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.2654 - accuracy: 0.8920 - val_loss: 0.2798 - val_accuracy: 0.8839 - lr: 2.5000e-04\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8880\n",
            "Epoch 24: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2684 - accuracy: 0.8880 - val_loss: 0.2803 - val_accuracy: 0.8859 - lr: 2.5000e-04\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.8834\n",
            "Epoch 25: val_accuracy improved from 0.88594 to 0.88854, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2731 - accuracy: 0.8834 - val_loss: 0.2806 - val_accuracy: 0.8885 - lr: 2.5000e-04\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8813Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 26: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2769 - accuracy: 0.8813 - val_loss: 0.2797 - val_accuracy: 0.8849 - lr: 2.5000e-04\n",
            "Epoch 26: early stopping\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2938 - accuracy: 0.8878\n",
            "Epoch 1: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 118s 2s/step - loss: 0.2938 - accuracy: 0.8878 - val_loss: 6.8432 - val_accuracy: 0.5932 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9252\n",
            "Epoch 2: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 108s 2s/step - loss: 0.1901 - accuracy: 0.9252 - val_loss: 7.0672 - val_accuracy: 0.5792 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9259\n",
            "Epoch 3: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.1842 - accuracy: 0.9259 - val_loss: 5.7760 - val_accuracy: 0.6255 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1707 - accuracy: 0.9334\n",
            "Epoch 4: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.1707 - accuracy: 0.9334 - val_loss: 1.3671 - val_accuracy: 0.7604 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9341\n",
            "Epoch 5: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1683 - accuracy: 0.9341 - val_loss: 3.4261 - val_accuracy: 0.6849 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2193 - accuracy: 0.9164\n",
            "Epoch 6: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 107s 2s/step - loss: 0.2193 - accuracy: 0.9164 - val_loss: 3.5768 - val_accuracy: 0.5365 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1712 - accuracy: 0.9313\n",
            "Epoch 7: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 110s 2s/step - loss: 0.1712 - accuracy: 0.9313 - val_loss: 3.3140 - val_accuracy: 0.5500 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "59/60 [============================>.] - ETA: 1s - loss: 0.1450 - accuracy: 0.9420\n",
            "Epoch 8: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 109s 2s/step - loss: 0.1449 - accuracy: 0.9420 - val_loss: 3.8639 - val_accuracy: 0.5495 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9424Restoring model weights from the end of the best epoch: 4.\n",
            "\n",
            "Epoch 9: val_accuracy did not improve from 0.88854\n",
            "60/60 [==============================] - 112s 2s/step - loss: 0.1417 - accuracy: 0.9424 - val_loss: 3.9260 - val_accuracy: 0.5354 - lr: 0.0010\n",
            "Epoch 9: early stopping\n",
            "Best validation accuracy: 0.7604166865348816\n",
            "Training with LR: 0.01, Optimizer: Adam, Fine-tuning at layer: 80\n",
            "Epoch 1/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4686 - accuracy: 0.7747\n",
            "Epoch 1: val_accuracy improved from -inf to 0.83698, saving model to best_model.h5\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.4686 - accuracy: 0.7747 - val_loss: 0.3730 - val_accuracy: 0.8370 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3603 - accuracy: 0.8421\n",
            "Epoch 2: val_accuracy improved from 0.83698 to 0.85208, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3603 - accuracy: 0.8421 - val_loss: 0.3344 - val_accuracy: 0.8521 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.8538\n",
            "Epoch 3: val_accuracy improved from 0.85208 to 0.86250, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3309 - accuracy: 0.8538 - val_loss: 0.3182 - val_accuracy: 0.8625 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3142 - accuracy: 0.8645\n",
            "Epoch 4: val_accuracy did not improve from 0.86250\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3142 - accuracy: 0.8645 - val_loss: 0.3311 - val_accuracy: 0.8578 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3048 - accuracy: 0.8695\n",
            "Epoch 5: val_accuracy improved from 0.86250 to 0.87135, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.3048 - accuracy: 0.8695 - val_loss: 0.3098 - val_accuracy: 0.8714 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8768\n",
            "Epoch 6: val_accuracy improved from 0.87135 to 0.87396, saving model to best_model.h5\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2929 - accuracy: 0.8768 - val_loss: 0.3044 - val_accuracy: 0.8740 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2968 - accuracy: 0.8736\n",
            "Epoch 7: val_accuracy improved from 0.87396 to 0.87760, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2968 - accuracy: 0.8736 - val_loss: 0.2999 - val_accuracy: 0.8776 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2844 - accuracy: 0.8816\n",
            "Epoch 8: val_accuracy did not improve from 0.87760\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2844 - accuracy: 0.8816 - val_loss: 0.2933 - val_accuracy: 0.8740 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2863 - accuracy: 0.8769\n",
            "Epoch 9: val_accuracy did not improve from 0.87760\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2863 - accuracy: 0.8769 - val_loss: 0.2945 - val_accuracy: 0.8745 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8829\n",
            "Epoch 10: val_accuracy improved from 0.87760 to 0.87813, saving model to best_model.h5\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2803 - accuracy: 0.8829 - val_loss: 0.2908 - val_accuracy: 0.8781 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2794 - accuracy: 0.8833\n",
            "Epoch 11: val_accuracy improved from 0.87813 to 0.87865, saving model to best_model.h5\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.2794 - accuracy: 0.8833 - val_loss: 0.2893 - val_accuracy: 0.8786 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2803 - accuracy: 0.8843\n",
            "Epoch 12: val_accuracy did not improve from 0.87865\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2803 - accuracy: 0.8843 - val_loss: 0.2877 - val_accuracy: 0.8786 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8863\n",
            "Epoch 13: val_accuracy did not improve from 0.87865\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2769 - accuracy: 0.8863 - val_loss: 0.2884 - val_accuracy: 0.8786 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2783 - accuracy: 0.8851\n",
            "Epoch 14: val_accuracy improved from 0.87865 to 0.87917, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2783 - accuracy: 0.8851 - val_loss: 0.2859 - val_accuracy: 0.8792 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2771 - accuracy: 0.8874\n",
            "Epoch 15: val_accuracy improved from 0.87917 to 0.87969, saving model to best_model.h5\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2771 - accuracy: 0.8874 - val_loss: 0.2849 - val_accuracy: 0.8797 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.8839\n",
            "Epoch 16: val_accuracy improved from 0.87969 to 0.88177, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2782 - accuracy: 0.8839 - val_loss: 0.2853 - val_accuracy: 0.8818 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.8817\n",
            "Epoch 17: val_accuracy did not improve from 0.88177\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2810 - accuracy: 0.8817 - val_loss: 0.2864 - val_accuracy: 0.8792 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2753 - accuracy: 0.8861\n",
            "Epoch 18: val_accuracy did not improve from 0.88177\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2753 - accuracy: 0.8861 - val_loss: 0.2852 - val_accuracy: 0.8807 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2704 - accuracy: 0.8858\n",
            "Epoch 19: val_accuracy improved from 0.88177 to 0.88385, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2704 - accuracy: 0.8858 - val_loss: 0.2831 - val_accuracy: 0.8839 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8842\n",
            "Epoch 20: val_accuracy did not improve from 0.88385\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2706 - accuracy: 0.8842 - val_loss: 0.2825 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2769 - accuracy: 0.8817\n",
            "Epoch 21: val_accuracy did not improve from 0.88385\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.2769 - accuracy: 0.8817 - val_loss: 0.2824 - val_accuracy: 0.8839 - lr: 2.5000e-04\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2754 - accuracy: 0.8813\n",
            "Epoch 22: val_accuracy did not improve from 0.88385\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.2754 - accuracy: 0.8813 - val_loss: 0.2819 - val_accuracy: 0.8828 - lr: 2.5000e-04\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2713 - accuracy: 0.8810\n",
            "Epoch 23: val_accuracy improved from 0.88385 to 0.88437, saving model to best_model.h5\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2713 - accuracy: 0.8810 - val_loss: 0.2816 - val_accuracy: 0.8844 - lr: 2.5000e-04\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8887\n",
            "Epoch 24: val_accuracy did not improve from 0.88437\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2715 - accuracy: 0.8887 - val_loss: 0.2820 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2687 - accuracy: 0.8876\n",
            "Epoch 25: val_accuracy improved from 0.88437 to 0.88490, saving model to best_model.h5\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2687 - accuracy: 0.8876 - val_loss: 0.2812 - val_accuracy: 0.8849 - lr: 2.5000e-04\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.8867\n",
            "Epoch 26: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.2701 - accuracy: 0.8867 - val_loss: 0.2824 - val_accuracy: 0.8844 - lr: 2.5000e-04\n",
            "Epoch 27/30\n",
            "59/60 [============================>.] - ETA: 1s - loss: 0.2662 - accuracy: 0.8888\n",
            "Epoch 27: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.2661 - accuracy: 0.8888 - val_loss: 0.2819 - val_accuracy: 0.8849 - lr: 2.5000e-04\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8896\n",
            "Epoch 28: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.2706 - accuracy: 0.8896 - val_loss: 0.2821 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2737 - accuracy: 0.8868\n",
            "Epoch 29: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 107s 2s/step - loss: 0.2737 - accuracy: 0.8868 - val_loss: 0.2811 - val_accuracy: 0.8844 - lr: 2.5000e-04\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.8876\n",
            "Epoch 30: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.2715 - accuracy: 0.8876 - val_loss: 0.2806 - val_accuracy: 0.8823 - lr: 1.2500e-04\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2959 - accuracy: 0.8825\n",
            "Epoch 1: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 121s 2s/step - loss: 0.2959 - accuracy: 0.8825 - val_loss: 13.2750 - val_accuracy: 0.6203 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9186\n",
            "Epoch 2: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.2030 - accuracy: 0.9186 - val_loss: 5.2218 - val_accuracy: 0.7521 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2028 - accuracy: 0.9206\n",
            "Epoch 3: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.2028 - accuracy: 0.9206 - val_loss: 2.5789 - val_accuracy: 0.7729 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9281\n",
            "Epoch 4: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.1842 - accuracy: 0.9281 - val_loss: 2.0047 - val_accuracy: 0.7729 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9333\n",
            "Epoch 5: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1633 - accuracy: 0.9333 - val_loss: 6.1724 - val_accuracy: 0.6573 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1481 - accuracy: 0.9407\n",
            "Epoch 6: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.1481 - accuracy: 0.9407 - val_loss: 2.2434 - val_accuracy: 0.7760 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1288 - accuracy: 0.9457\n",
            "Epoch 7: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 107s 2s/step - loss: 0.1288 - accuracy: 0.9457 - val_loss: 2.6730 - val_accuracy: 0.7729 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9469\n",
            "Epoch 8: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.1350 - accuracy: 0.9469 - val_loss: 2.0368 - val_accuracy: 0.7630 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1690 - accuracy: 0.9307\n",
            "Epoch 9: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.1690 - accuracy: 0.9307 - val_loss: 1.7518 - val_accuracy: 0.7349 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1237 - accuracy: 0.9505\n",
            "Epoch 10: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.1237 - accuracy: 0.9505 - val_loss: 0.8717 - val_accuracy: 0.8568 - lr: 5.0000e-04\n",
            "Epoch 11/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1206 - accuracy: 0.9551\n",
            "Epoch 11: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.1206 - accuracy: 0.9551 - val_loss: 0.8387 - val_accuracy: 0.8745 - lr: 5.0000e-04\n",
            "Epoch 12/20\n",
            "59/60 [============================>.] - ETA: 1s - loss: 0.0931 - accuracy: 0.9641\n",
            "Epoch 12: val_accuracy improved from 0.88490 to 0.88698, saving model to best_model.h5\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.0930 - accuracy: 0.9641 - val_loss: 0.7352 - val_accuracy: 0.8870 - lr: 5.0000e-04\n",
            "Epoch 13/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9664\n",
            "Epoch 13: val_accuracy improved from 0.88698 to 0.89583, saving model to best_model.h5\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.0875 - accuracy: 0.9664 - val_loss: 0.4707 - val_accuracy: 0.8958 - lr: 5.0000e-04\n",
            "Epoch 14/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9655\n",
            "Epoch 14: val_accuracy did not improve from 0.89583\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.0834 - accuracy: 0.9655 - val_loss: 0.5424 - val_accuracy: 0.8771 - lr: 5.0000e-04\n",
            "Epoch 15/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9713\n",
            "Epoch 15: val_accuracy did not improve from 0.89583\n",
            "60/60 [==============================] - 114s 2s/step - loss: 0.0706 - accuracy: 0.9713 - val_loss: 0.7541 - val_accuracy: 0.7953 - lr: 5.0000e-04\n",
            "Epoch 16/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9725\n",
            "Epoch 16: val_accuracy did not improve from 0.89583\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.0714 - accuracy: 0.9725 - val_loss: 0.6464 - val_accuracy: 0.8453 - lr: 5.0000e-04\n",
            "Epoch 17/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9722\n",
            "Epoch 17: val_accuracy did not improve from 0.89583\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.0699 - accuracy: 0.9722 - val_loss: 0.6297 - val_accuracy: 0.8167 - lr: 5.0000e-04\n",
            "Epoch 18/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9530\n",
            "Epoch 18: val_accuracy improved from 0.89583 to 0.90625, saving model to best_model.h5\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1366 - accuracy: 0.9530 - val_loss: 0.3833 - val_accuracy: 0.9062 - lr: 5.0000e-04\n",
            "Epoch 19/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1287 - accuracy: 0.9510\n",
            "Epoch 19: val_accuracy did not improve from 0.90625\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1287 - accuracy: 0.9510 - val_loss: 1.0058 - val_accuracy: 0.8031 - lr: 5.0000e-04\n",
            "Epoch 20/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.0994 - accuracy: 0.9618\n",
            "Epoch 20: val_accuracy did not improve from 0.90625\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.0994 - accuracy: 0.9618 - val_loss: 0.6090 - val_accuracy: 0.8141 - lr: 2.5000e-04\n",
            "Best validation accuracy: 0.90625\n",
            "Training with LR: 0.01, Optimizer: Adam, Fine-tuning at layer: 110\n",
            "Epoch 1/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.4659 - accuracy: 0.7785\n",
            "Epoch 1: val_accuracy improved from -inf to 0.84948, saving model to best_model.h5\n",
            "60/60 [==============================] - 105s 2s/step - loss: 0.4659 - accuracy: 0.7785 - val_loss: 0.3480 - val_accuracy: 0.8495 - lr: 0.0010\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3508 - accuracy: 0.8454\n",
            "Epoch 2: val_accuracy improved from 0.84948 to 0.85990, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3508 - accuracy: 0.8454 - val_loss: 0.3297 - val_accuracy: 0.8599 - lr: 0.0010\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3227 - accuracy: 0.8641\n",
            "Epoch 3: val_accuracy improved from 0.85990 to 0.86406, saving model to best_model.h5\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.3227 - accuracy: 0.8641 - val_loss: 0.3115 - val_accuracy: 0.8641 - lr: 0.0010\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3147 - accuracy: 0.8646\n",
            "Epoch 4: val_accuracy improved from 0.86406 to 0.86510, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3147 - accuracy: 0.8646 - val_loss: 0.3170 - val_accuracy: 0.8651 - lr: 0.0010\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3000 - accuracy: 0.8747\n",
            "Epoch 5: val_accuracy improved from 0.86510 to 0.87344, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3000 - accuracy: 0.8747 - val_loss: 0.3017 - val_accuracy: 0.8734 - lr: 0.0010\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8722\n",
            "Epoch 6: val_accuracy improved from 0.87344 to 0.87865, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.3008 - accuracy: 0.8722 - val_loss: 0.2972 - val_accuracy: 0.8786 - lr: 0.0010\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2953 - accuracy: 0.8782\n",
            "Epoch 7: val_accuracy did not improve from 0.87865\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2953 - accuracy: 0.8782 - val_loss: 0.2955 - val_accuracy: 0.8771 - lr: 0.0010\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2858 - accuracy: 0.8760\n",
            "Epoch 8: val_accuracy improved from 0.87865 to 0.88177, saving model to best_model.h5\n",
            "60/60 [==============================] - 109s 2s/step - loss: 0.2858 - accuracy: 0.8760 - val_loss: 0.2928 - val_accuracy: 0.8818 - lr: 0.0010\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2846 - accuracy: 0.8813\n",
            "Epoch 9: val_accuracy improved from 0.88177 to 0.88333, saving model to best_model.h5\n",
            "60/60 [==============================] - 104s 2s/step - loss: 0.2846 - accuracy: 0.8813 - val_loss: 0.2900 - val_accuracy: 0.8833 - lr: 0.0010\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.8838\n",
            "Epoch 10: val_accuracy improved from 0.88333 to 0.88490, saving model to best_model.h5\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2741 - accuracy: 0.8838 - val_loss: 0.2883 - val_accuracy: 0.8849 - lr: 5.0000e-04\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2740 - accuracy: 0.8826\n",
            "Epoch 11: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2740 - accuracy: 0.8826 - val_loss: 0.2870 - val_accuracy: 0.8797 - lr: 5.0000e-04\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2777 - accuracy: 0.8871\n",
            "Epoch 12: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2777 - accuracy: 0.8871 - val_loss: 0.3000 - val_accuracy: 0.8724 - lr: 5.0000e-04\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.8823\n",
            "Epoch 13: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 99s 2s/step - loss: 0.2784 - accuracy: 0.8823 - val_loss: 0.2911 - val_accuracy: 0.8776 - lr: 5.0000e-04\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.8780\n",
            "Epoch 14: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2808 - accuracy: 0.8780 - val_loss: 0.2854 - val_accuracy: 0.8797 - lr: 5.0000e-04\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2810 - accuracy: 0.8835\n",
            "Epoch 15: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2810 - accuracy: 0.8835 - val_loss: 0.2877 - val_accuracy: 0.8797 - lr: 5.0000e-04\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2721 - accuracy: 0.8882\n",
            "Epoch 16: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2721 - accuracy: 0.8882 - val_loss: 0.2851 - val_accuracy: 0.8802 - lr: 5.0000e-04\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2694 - accuracy: 0.8868\n",
            "Epoch 17: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2694 - accuracy: 0.8868 - val_loss: 0.2832 - val_accuracy: 0.8833 - lr: 5.0000e-04\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2722 - accuracy: 0.8863\n",
            "Epoch 18: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2722 - accuracy: 0.8863 - val_loss: 0.2832 - val_accuracy: 0.8844 - lr: 5.0000e-04\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2747 - accuracy: 0.8838\n",
            "Epoch 19: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2747 - accuracy: 0.8838 - val_loss: 0.2826 - val_accuracy: 0.8813 - lr: 5.0000e-04\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8902\n",
            "Epoch 20: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2671 - accuracy: 0.8902 - val_loss: 0.2819 - val_accuracy: 0.8823 - lr: 2.5000e-04\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2706 - accuracy: 0.8884\n",
            "Epoch 21: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2706 - accuracy: 0.8884 - val_loss: 0.2811 - val_accuracy: 0.8839 - lr: 2.5000e-04\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.8915\n",
            "Epoch 22: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2646 - accuracy: 0.8915 - val_loss: 0.2815 - val_accuracy: 0.8823 - lr: 2.5000e-04\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2787 - accuracy: 0.8849\n",
            "Epoch 23: val_accuracy did not improve from 0.88490\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.2787 - accuracy: 0.8849 - val_loss: 0.2818 - val_accuracy: 0.8839 - lr: 2.5000e-04\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2688 - accuracy: 0.8867\n",
            "Epoch 24: val_accuracy improved from 0.88490 to 0.88594, saving model to best_model.h5\n",
            "60/60 [==============================] - 100s 2s/step - loss: 0.2688 - accuracy: 0.8867 - val_loss: 0.2824 - val_accuracy: 0.8859 - lr: 2.5000e-04\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2681 - accuracy: 0.8900\n",
            "Epoch 25: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2681 - accuracy: 0.8900 - val_loss: 0.2812 - val_accuracy: 0.8833 - lr: 2.5000e-04\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2724 - accuracy: 0.8858Restoring model weights from the end of the best epoch: 21.\n",
            "\n",
            "Epoch 26: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 102s 2s/step - loss: 0.2724 - accuracy: 0.8858 - val_loss: 0.2832 - val_accuracy: 0.8797 - lr: 2.5000e-04\n",
            "Epoch 26: early stopping\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.8650\n",
            "Epoch 1: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 111s 2s/step - loss: 0.3154 - accuracy: 0.8650 - val_loss: 10.0450 - val_accuracy: 0.5255 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.2079 - accuracy: 0.9190\n",
            "Epoch 2: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 101s 2s/step - loss: 0.2079 - accuracy: 0.9190 - val_loss: 9.3306 - val_accuracy: 0.5219 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1777 - accuracy: 0.9288\n",
            "Epoch 3: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 106s 2s/step - loss: 0.1777 - accuracy: 0.9288 - val_loss: 14.1704 - val_accuracy: 0.5203 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "60/60 [==============================] - ETA: 0s - loss: 0.1830 - accuracy: 0.9276\n",
            "Epoch 4: val_accuracy did not improve from 0.88594\n",
            "60/60 [==============================] - 103s 2s/step - loss: 0.1830 - accuracy: 0.9276 - val_loss: 9.8878 - val_accuracy: 0.5208 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "22/60 [==========>...................] - ETA: 1:05 - loss: 0.1870 - accuracy: 0.9264"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "\n",
        "\n",
        "# Load MobileNetV2 as the base model, excluding the top (classification) layer\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "# Create the model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')  # Binary classification layer\n",
        "])\n",
        "\n",
        "model.compile(optimizer=RMSprop(learning_rate=1e-2),  # Start with a learning rate of 0.001\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()  # Optional, to see the model architecture\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UOsnVYvOcoS",
        "outputId": "9bdbd5c3-7119-46ba-8468-7700b6f6478a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Func  (None, 7, 7, 1280)        2257984   \n",
            " tional)                                                         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1280)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 1281      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2259265 (8.62 MB)\n",
            "Trainable params: 1281 (5.00 KB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
        "\n",
        "# Model Checkpoint\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "# Function for step decay schedule\n",
        "def step_decay(epoch):\n",
        "    initial_lr = 1e-2  # Starting learning rate\n",
        "    drop = 0.5  # Reduce learning rate by half\n",
        "    epochs_drop = 10.0  # Reduce every 10 epochs\n",
        "    lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "    return lr\n",
        "\n",
        "# Define the Early Stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',  # Monitor validation loss\n",
        "    patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
        "    restore_best_weights=True)  # Restore model weights from the epoch with the best value of the monitored metric\n",
        "\n",
        "# Define the Learning Rate Scheduler callback\n",
        "lr_scheduler = LearningRateScheduler(step_decay)\n",
        "\n",
        "# Add both callbacks to the callbacks list in model.fit()\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "    epochs=50,  # Increased epochs for fine-tuning and allowing the learning rate scheduler to adjust the learning rate\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint])  # Including both callbacks here\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N89fuD53BAeC",
        "outputId": "d69fb3fd-d2b1-4a15-a200-eea4e9402a12"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.6325 - accuracy: 0.7841\n",
            "Epoch 1: val_accuracy improved from -inf to 0.85677, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/120 [==============================] - 133s 924ms/step - loss: 0.6325 - accuracy: 0.7841 - val_loss: 0.3856 - val_accuracy: 0.8568 - lr: 0.0100\n",
            "Epoch 2/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4718 - accuracy: 0.8360\n",
            "Epoch 2: val_accuracy did not improve from 0.85677\n",
            "120/120 [==============================] - 107s 891ms/step - loss: 0.4718 - accuracy: 0.8360 - val_loss: 0.8220 - val_accuracy: 0.7146 - lr: 0.0100\n",
            "Epoch 3/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.8392\n",
            "Epoch 3: val_accuracy did not improve from 0.85677\n",
            "120/120 [==============================] - 105s 872ms/step - loss: 0.4715 - accuracy: 0.8392 - val_loss: 1.0895 - val_accuracy: 0.7437 - lr: 0.0100\n",
            "Epoch 4/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4920 - accuracy: 0.8400\n",
            "Epoch 4: val_accuracy did not improve from 0.85677\n",
            "120/120 [==============================] - 105s 876ms/step - loss: 0.4920 - accuracy: 0.8400 - val_loss: 0.4540 - val_accuracy: 0.8490 - lr: 0.0100\n",
            "Epoch 5/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.8487\n",
            "Epoch 5: val_accuracy improved from 0.85677 to 0.86979, saving model to best_model.h5\n",
            "120/120 [==============================] - 105s 874ms/step - loss: 0.4673 - accuracy: 0.8487 - val_loss: 0.3846 - val_accuracy: 0.8698 - lr: 0.0100\n",
            "Epoch 6/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4590 - accuracy: 0.8450\n",
            "Epoch 6: val_accuracy did not improve from 0.86979\n",
            "120/120 [==============================] - 104s 867ms/step - loss: 0.4590 - accuracy: 0.8450 - val_loss: 0.4282 - val_accuracy: 0.8411 - lr: 0.0100\n",
            "Epoch 7/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4776 - accuracy: 0.8433\n",
            "Epoch 7: val_accuracy improved from 0.86979 to 0.87813, saving model to best_model.h5\n",
            "120/120 [==============================] - 106s 881ms/step - loss: 0.4776 - accuracy: 0.8433 - val_loss: 0.3449 - val_accuracy: 0.8781 - lr: 0.0100\n",
            "Epoch 8/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4714 - accuracy: 0.8444\n",
            "Epoch 8: val_accuracy did not improve from 0.87813\n",
            "120/120 [==============================] - 103s 858ms/step - loss: 0.4714 - accuracy: 0.8444 - val_loss: 0.3809 - val_accuracy: 0.8630 - lr: 0.0100\n",
            "Epoch 9/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4455 - accuracy: 0.8542\n",
            "Epoch 9: val_accuracy did not improve from 0.87813\n",
            "120/120 [==============================] - 104s 867ms/step - loss: 0.4455 - accuracy: 0.8542 - val_loss: 0.3578 - val_accuracy: 0.8771 - lr: 0.0100\n",
            "Epoch 10/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3415 - accuracy: 0.8723\n",
            "Epoch 10: val_accuracy improved from 0.87813 to 0.88490, saving model to best_model.h5\n",
            "120/120 [==============================] - 104s 871ms/step - loss: 0.3415 - accuracy: 0.8723 - val_loss: 0.2919 - val_accuracy: 0.8849 - lr: 0.0050\n",
            "Epoch 11/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3206 - accuracy: 0.8770\n",
            "Epoch 11: val_accuracy did not improve from 0.88490\n",
            "120/120 [==============================] - 103s 856ms/step - loss: 0.3206 - accuracy: 0.8770 - val_loss: 0.3872 - val_accuracy: 0.8583 - lr: 0.0050\n",
            "Epoch 12/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3254 - accuracy: 0.8735\n",
            "Epoch 12: val_accuracy did not improve from 0.88490\n",
            "120/120 [==============================] - 105s 877ms/step - loss: 0.3254 - accuracy: 0.8735 - val_loss: 0.2899 - val_accuracy: 0.8833 - lr: 0.0050\n",
            "Epoch 13/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3410 - accuracy: 0.8703\n",
            "Epoch 13: val_accuracy improved from 0.88490 to 0.88594, saving model to best_model.h5\n",
            "120/120 [==============================] - 105s 878ms/step - loss: 0.3410 - accuracy: 0.8703 - val_loss: 0.2858 - val_accuracy: 0.8859 - lr: 0.0050\n",
            "Epoch 14/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3240 - accuracy: 0.8722\n",
            "Epoch 14: val_accuracy did not improve from 0.88594\n",
            "120/120 [==============================] - 103s 859ms/step - loss: 0.3240 - accuracy: 0.8722 - val_loss: 0.2940 - val_accuracy: 0.8844 - lr: 0.0050\n",
            "Epoch 15/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.8744\n",
            "Epoch 15: val_accuracy improved from 0.88594 to 0.89010, saving model to best_model.h5\n",
            "120/120 [==============================] - 104s 871ms/step - loss: 0.3154 - accuracy: 0.8744 - val_loss: 0.2923 - val_accuracy: 0.8901 - lr: 0.0050\n",
            "Epoch 16/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3176 - accuracy: 0.8761\n",
            "Epoch 16: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 102s 853ms/step - loss: 0.3176 - accuracy: 0.8761 - val_loss: 0.3018 - val_accuracy: 0.8776 - lr: 0.0050\n",
            "Epoch 17/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.8706\n",
            "Epoch 17: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 103s 862ms/step - loss: 0.3317 - accuracy: 0.8706 - val_loss: 0.3178 - val_accuracy: 0.8766 - lr: 0.0050\n",
            "Epoch 18/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3238 - accuracy: 0.8749\n",
            "Epoch 18: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 104s 870ms/step - loss: 0.3238 - accuracy: 0.8749 - val_loss: 0.2878 - val_accuracy: 0.8839 - lr: 0.0050\n",
            "Epoch 19/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8782\n",
            "Epoch 19: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 103s 858ms/step - loss: 0.3127 - accuracy: 0.8782 - val_loss: 0.2952 - val_accuracy: 0.8854 - lr: 0.0050\n",
            "Epoch 20/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2981 - accuracy: 0.8787\n",
            "Epoch 20: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 110s 915ms/step - loss: 0.2981 - accuracy: 0.8787 - val_loss: 0.2781 - val_accuracy: 0.8891 - lr: 0.0025\n",
            "Epoch 21/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2770 - accuracy: 0.8895\n",
            "Epoch 21: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 105s 873ms/step - loss: 0.2770 - accuracy: 0.8895 - val_loss: 0.2776 - val_accuracy: 0.8859 - lr: 0.0025\n",
            "Epoch 22/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2760 - accuracy: 0.8849\n",
            "Epoch 22: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 103s 858ms/step - loss: 0.2760 - accuracy: 0.8849 - val_loss: 0.2974 - val_accuracy: 0.8833 - lr: 0.0025\n",
            "Epoch 23/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8870\n",
            "Epoch 23: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 104s 868ms/step - loss: 0.2866 - accuracy: 0.8870 - val_loss: 0.2806 - val_accuracy: 0.8854 - lr: 0.0025\n",
            "Epoch 24/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8875\n",
            "Epoch 24: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 108s 900ms/step - loss: 0.2767 - accuracy: 0.8875 - val_loss: 0.2796 - val_accuracy: 0.8880 - lr: 0.0025\n",
            "Epoch 25/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.8854\n",
            "Epoch 25: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 105s 876ms/step - loss: 0.2782 - accuracy: 0.8854 - val_loss: 0.2872 - val_accuracy: 0.8839 - lr: 0.0025\n",
            "Epoch 26/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2914 - accuracy: 0.8808\n",
            "Epoch 26: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 103s 859ms/step - loss: 0.2914 - accuracy: 0.8808 - val_loss: 0.2935 - val_accuracy: 0.8833 - lr: 0.0025\n",
            "Epoch 27/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8856\n",
            "Epoch 27: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 105s 872ms/step - loss: 0.2879 - accuracy: 0.8856 - val_loss: 0.2772 - val_accuracy: 0.8896 - lr: 0.0025\n",
            "Epoch 28/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2817 - accuracy: 0.8837\n",
            "Epoch 28: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 105s 873ms/step - loss: 0.2817 - accuracy: 0.8837 - val_loss: 0.3353 - val_accuracy: 0.8651 - lr: 0.0025\n",
            "Epoch 29/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2759 - accuracy: 0.8878\n",
            "Epoch 29: val_accuracy improved from 0.89010 to 0.89062, saving model to best_model.h5\n",
            "120/120 [==============================] - 104s 861ms/step - loss: 0.2759 - accuracy: 0.8878 - val_loss: 0.2718 - val_accuracy: 0.8906 - lr: 0.0025\n",
            "Epoch 30/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.8886\n",
            "Epoch 30: val_accuracy improved from 0.89062 to 0.89688, saving model to best_model.h5\n",
            "120/120 [==============================] - 105s 878ms/step - loss: 0.2768 - accuracy: 0.8886 - val_loss: 0.2697 - val_accuracy: 0.8969 - lr: 0.0012\n",
            "Epoch 31/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2623 - accuracy: 0.8959\n",
            "Epoch 31: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 104s 869ms/step - loss: 0.2623 - accuracy: 0.8959 - val_loss: 0.2686 - val_accuracy: 0.8948 - lr: 0.0012\n",
            "Epoch 32/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.8888\n",
            "Epoch 32: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 106s 884ms/step - loss: 0.2684 - accuracy: 0.8888 - val_loss: 0.2701 - val_accuracy: 0.8901 - lr: 0.0012\n",
            "Epoch 33/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.8927\n",
            "Epoch 33: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 104s 865ms/step - loss: 0.2701 - accuracy: 0.8927 - val_loss: 0.2738 - val_accuracy: 0.8901 - lr: 0.0012\n",
            "Epoch 34/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.8924\n",
            "Epoch 34: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 103s 857ms/step - loss: 0.2662 - accuracy: 0.8924 - val_loss: 0.2675 - val_accuracy: 0.8885 - lr: 0.0012\n",
            "Epoch 35/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2709 - accuracy: 0.8911\n",
            "Epoch 35: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 106s 881ms/step - loss: 0.2709 - accuracy: 0.8911 - val_loss: 0.2766 - val_accuracy: 0.8932 - lr: 0.0012\n",
            "Epoch 36/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2702 - accuracy: 0.8904\n",
            "Epoch 36: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 103s 860ms/step - loss: 0.2702 - accuracy: 0.8904 - val_loss: 0.2721 - val_accuracy: 0.8865 - lr: 0.0012\n",
            "Epoch 37/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2648 - accuracy: 0.8949\n",
            "Epoch 37: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 104s 871ms/step - loss: 0.2648 - accuracy: 0.8949 - val_loss: 0.2727 - val_accuracy: 0.8885 - lr: 0.0012\n",
            "Epoch 38/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2679 - accuracy: 0.8938\n",
            "Epoch 38: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 103s 861ms/step - loss: 0.2679 - accuracy: 0.8938 - val_loss: 0.2703 - val_accuracy: 0.8875 - lr: 0.0012\n",
            "Epoch 39/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.8933\n",
            "Epoch 39: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 109s 908ms/step - loss: 0.2630 - accuracy: 0.8933 - val_loss: 0.2765 - val_accuracy: 0.8870 - lr: 0.0012\n",
            "Epoch 40/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.8913\n",
            "Epoch 40: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 109s 907ms/step - loss: 0.2627 - accuracy: 0.8913 - val_loss: 0.2703 - val_accuracy: 0.8901 - lr: 6.2500e-04\n",
            "Epoch 41/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.8894\n",
            "Epoch 41: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 103s 861ms/step - loss: 0.2673 - accuracy: 0.8894 - val_loss: 0.2722 - val_accuracy: 0.8875 - lr: 6.2500e-04\n",
            "Epoch 42/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2584 - accuracy: 0.8920\n",
            "Epoch 42: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 104s 870ms/step - loss: 0.2584 - accuracy: 0.8920 - val_loss: 0.2684 - val_accuracy: 0.8880 - lr: 6.2500e-04\n",
            "Epoch 43/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.8912\n",
            "Epoch 43: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 102s 851ms/step - loss: 0.2641 - accuracy: 0.8912 - val_loss: 0.2683 - val_accuracy: 0.8901 - lr: 6.2500e-04\n",
            "Epoch 44/50\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.8959\n",
            "Epoch 44: val_accuracy did not improve from 0.89688\n",
            "120/120 [==============================] - 104s 865ms/step - loss: 0.2577 - accuracy: 0.8959 - val_loss: 0.2679 - val_accuracy: 0.8927 - lr: 6.2500e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')\n",
        "\n",
        "def fine_tune_model(model, base_model, fine_tune_start_layer=100):\n",
        "    \"\"\"\n",
        "    Unfreeze the top layers of the model and compile the model for fine-tuning.\n",
        "    \"\"\"\n",
        "    base_model.trainable = True\n",
        "\n",
        "    # Freeze all the layers before the `fine_tune_start_layer`\n",
        "    for layer in base_model.layers[:fine_tune_start_layer]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=1e-5),  # Lower learning rate\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "if test_acc < 0.95:\n",
        "    fine_tune_at = 154  # Experiment with this value\n",
        "    model_fine = fine_tune_model(model, base_model, fine_tune_at)\n",
        "\n",
        "    def step_decay(epoch):\n",
        "        initial_lr = 1e-2  # Starting learning rate\n",
        "        drop = 0.5  # Reduce learning rate by half\n",
        "        epochs_drop = 10.0  # Reduce every 10 epochs\n",
        "        lr = initial_lr * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
        "        return lr\n",
        "\n",
        "    # Define early stopping and learning rate scheduler\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(step_decay)\n",
        "\n",
        "\n",
        "\n",
        "    # Model Checkpoint\n",
        "    model_fine_checkpoint = ModelCheckpoint('fine_best_model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1)\n",
        "\n",
        "\n",
        "    history_fine = model_fine.fit(train_generator,\n",
        "                             steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
        "                             epochs=20,  # You might need more epochs\n",
        "                             validation_data=validation_generator,\n",
        "                             validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
        "                             callbacks=[early_stopping, lr_scheduler, model_fine_checkpoint])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvoK2OTCOwYl",
        "outputId": "46a4b7a3-cbec-47b5-b0ab-6c6dc7711eac"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 8s 183ms/step - loss: 0.9091 - accuracy: 0.5042\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.RMSprop.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 50.42%\n",
            "Epoch 1/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.6203 - accuracy: 0.7869\n",
            "Epoch 1: val_accuracy improved from -inf to 0.83281, saving model to fine_best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r120/120 [==============================] - 121s 969ms/step - loss: 0.6203 - accuracy: 0.7869 - val_loss: 0.5273 - val_accuracy: 0.8328 - lr: 0.0100\n",
            "Epoch 2/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4998 - accuracy: 0.8287\n",
            "Epoch 2: val_accuracy did not improve from 0.83281\n",
            "120/120 [==============================] - 112s 931ms/step - loss: 0.4998 - accuracy: 0.8287 - val_loss: 0.5857 - val_accuracy: 0.7880 - lr: 0.0100\n",
            "Epoch 3/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4796 - accuracy: 0.8400\n",
            "Epoch 3: val_accuracy did not improve from 0.83281\n",
            "120/120 [==============================] - 113s 939ms/step - loss: 0.4796 - accuracy: 0.8400 - val_loss: 0.6055 - val_accuracy: 0.7922 - lr: 0.0100\n",
            "Epoch 4/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.8471\n",
            "Epoch 4: val_accuracy improved from 0.83281 to 0.86771, saving model to fine_best_model.h5\n",
            "120/120 [==============================] - 112s 938ms/step - loss: 0.4618 - accuracy: 0.8471 - val_loss: 0.3925 - val_accuracy: 0.8677 - lr: 0.0100\n",
            "Epoch 5/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4684 - accuracy: 0.8420\n",
            "Epoch 5: val_accuracy improved from 0.86771 to 0.89010, saving model to fine_best_model.h5\n",
            "120/120 [==============================] - 113s 941ms/step - loss: 0.4684 - accuracy: 0.8420 - val_loss: 0.3004 - val_accuracy: 0.8901 - lr: 0.0100\n",
            "Epoch 6/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4599 - accuracy: 0.8512\n",
            "Epoch 6: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 112s 933ms/step - loss: 0.4599 - accuracy: 0.8512 - val_loss: 0.5143 - val_accuracy: 0.8099 - lr: 0.0100\n",
            "Epoch 7/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4566 - accuracy: 0.8459\n",
            "Epoch 7: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 113s 941ms/step - loss: 0.4566 - accuracy: 0.8459 - val_loss: 0.3103 - val_accuracy: 0.8880 - lr: 0.0100\n",
            "Epoch 8/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.8545\n",
            "Epoch 8: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 111s 925ms/step - loss: 0.4459 - accuracy: 0.8545 - val_loss: 0.3132 - val_accuracy: 0.8880 - lr: 0.0100\n",
            "Epoch 9/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.4528 - accuracy: 0.8479\n",
            "Epoch 9: val_accuracy did not improve from 0.89010\n",
            "120/120 [==============================] - 112s 932ms/step - loss: 0.4528 - accuracy: 0.8479 - val_loss: 0.3122 - val_accuracy: 0.8844 - lr: 0.0100\n",
            "Epoch 10/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3314 - accuracy: 0.8743\n",
            "Epoch 10: val_accuracy improved from 0.89010 to 0.89323, saving model to fine_best_model.h5\n",
            "120/120 [==============================] - 119s 988ms/step - loss: 0.3314 - accuracy: 0.8743 - val_loss: 0.2856 - val_accuracy: 0.8932 - lr: 0.0050\n",
            "Epoch 11/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3414 - accuracy: 0.8709\n",
            "Epoch 11: val_accuracy did not improve from 0.89323\n",
            "120/120 [==============================] - 112s 932ms/step - loss: 0.3414 - accuracy: 0.8709 - val_loss: 0.2789 - val_accuracy: 0.8870 - lr: 0.0050\n",
            "Epoch 12/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3451 - accuracy: 0.8650\n",
            "Epoch 12: val_accuracy did not improve from 0.89323\n",
            "120/120 [==============================] - 111s 929ms/step - loss: 0.3451 - accuracy: 0.8650 - val_loss: 0.2925 - val_accuracy: 0.8870 - lr: 0.0050\n",
            "Epoch 13/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8692\n",
            "Epoch 13: val_accuracy improved from 0.89323 to 0.89583, saving model to fine_best_model.h5\n",
            "120/120 [==============================] - 117s 976ms/step - loss: 0.3306 - accuracy: 0.8692 - val_loss: 0.2795 - val_accuracy: 0.8958 - lr: 0.0050\n",
            "Epoch 14/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3183 - accuracy: 0.8764\n",
            "Epoch 14: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 112s 934ms/step - loss: 0.3183 - accuracy: 0.8764 - val_loss: 0.2869 - val_accuracy: 0.8901 - lr: 0.0050\n",
            "Epoch 15/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8730\n",
            "Epoch 15: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 112s 932ms/step - loss: 0.3243 - accuracy: 0.8730 - val_loss: 0.2917 - val_accuracy: 0.8859 - lr: 0.0050\n",
            "Epoch 16/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8698\n",
            "Epoch 16: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 112s 937ms/step - loss: 0.3336 - accuracy: 0.8698 - val_loss: 0.3496 - val_accuracy: 0.8656 - lr: 0.0050\n",
            "Epoch 17/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.8793\n",
            "Epoch 17: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 112s 938ms/step - loss: 0.3154 - accuracy: 0.8793 - val_loss: 0.3011 - val_accuracy: 0.8792 - lr: 0.0050\n",
            "Epoch 18/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3127 - accuracy: 0.8780\n",
            "Epoch 18: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 111s 930ms/step - loss: 0.3127 - accuracy: 0.8780 - val_loss: 0.3093 - val_accuracy: 0.8833 - lr: 0.0050\n",
            "Epoch 19/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.3217 - accuracy: 0.8780\n",
            "Epoch 19: val_accuracy did not improve from 0.89583\n",
            "120/120 [==============================] - 112s 932ms/step - loss: 0.3217 - accuracy: 0.8780 - val_loss: 0.5055 - val_accuracy: 0.8427 - lr: 0.0050\n",
            "Epoch 20/20\n",
            "120/120 [==============================] - ETA: 0s - loss: 0.2943 - accuracy: 0.8822\n",
            "Epoch 20: val_accuracy improved from 0.89583 to 0.89740, saving model to fine_best_model.h5\n",
            "120/120 [==============================] - 112s 934ms/step - loss: 0.2943 - accuracy: 0.8822 - val_loss: 0.2615 - val_accuracy: 0.8974 - lr: 0.0025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_fine.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyMIvHXyBB6W",
        "outputId": "dc7af56f-5193-4b82-abfd-e083248bb605"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 3s 162ms/step - loss: 0.2405 - accuracy: 0.9104\n",
            "Test accuracy: 91.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_fine)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model\n",
        "with open('model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n"
      ],
      "metadata": {
        "id": "YTi1Ril0QFyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f680287-022e-4b4f-d159-b833d12fa4e2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
            "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n"
          ]
        }
      ]
    }
  ]
}